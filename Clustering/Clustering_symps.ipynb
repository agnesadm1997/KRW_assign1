{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import rdflib\n",
    "from rdflib import Graph, Literal, Namespace, RDF, URIRef, OWL\n",
    "from rdflib.extras.external_graph_libs import rdflib_to_networkx_digraph\n",
    "from rdflib.namespace import DC, FOAF\n",
    "\n",
    "from owlready2 import *\n",
    "from owlready2 import get_ontology\n",
    "\n",
    "import networkx as nx\n",
    "import networkx.algorithms.community as nx_comm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               subject  \\\n",
      "0  http://www.medsur.org/patient_10507   \n",
      "1  http://www.medsur.org/patient_10594   \n",
      "2   http://www.medsur.org/patient_4479   \n",
      "3  http://www.medsur.org/patient_13760   \n",
      "4   http://www.medsur.org/patient_2136   \n",
      "\n",
      "                                   predicate  \\\n",
      "0   http://example.org/medsur.rdf#hasOutcome   \n",
      "1       http://example.org/medsur.rdf#hasLLT   \n",
      "2  http://example.org/medsur.rdf#hasAgeGroup   \n",
      "3      http://example.org/medsur.rdf#hasHLGT   \n",
      "4       http://example.org/medsur.rdf#hasSOC   \n",
      "\n",
      "                                   object  \n",
      "0   http://www.medsur.org/outcome/Ongoing  \n",
      "1  http://www.medsur.org/symptom/10046901  \n",
      "2         http://www.medsur.org/age/25_44  \n",
      "3     http://www.medsur.org/hlgt/10018424  \n",
      "4      http://www.medsur.org/soc/10007541  \n"
     ]
    }
   ],
   "source": [
    "# load csv medsur.csv\n",
    "colnames = [\"subject\", \"predicate\", \"object\"]\n",
    "triples_df = pd.read_csv('medsur.csv', names=colnames, header=None)\n",
    "\n",
    "triples_df['object'] = triples_df['object'].str.rstrip()\n",
    "triples_df['subject'] = triples_df['subject'].str.rstrip() \n",
    "triples_df['predicate'] = triples_df['predicate'].str.rstrip() \n",
    "\n",
    "triples_df = triples_df.drop_duplicates()\n",
    "triples_df = triples_df.dropna()\n",
    "\n",
    "# filter out all triples that contain /nan\n",
    "triples_df = triples_df[~triples_df['subject'].str.contains(\"/nan\")]\n",
    "triples_df = triples_df[~triples_df['object'].str.contains(\"/nan\")]\n",
    "\n",
    "print(triples_df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter triples to get symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                subject                             predicate  \\\n",
      "1   http://www.medsur.org/patient_10594  http://example.org/medsur.rdf#hasLLT   \n",
      "6   http://www.medsur.org/patient_12752   http://example.org/medsur.rdf#hasPT   \n",
      "14    http://www.medsur.org/patient_418   http://example.org/medsur.rdf#hasPT   \n",
      "17  http://www.medsur.org/patient_13840  http://example.org/medsur.rdf#hasLLT   \n",
      "20    http://www.medsur.org/patient_988   http://example.org/medsur.rdf#hasPT   \n",
      "\n",
      "                                    object  \n",
      "1   http://www.medsur.org/symptom/10046901  \n",
      "6   http://www.medsur.org/symptom/10016334  \n",
      "14  http://www.medsur.org/symptom/10017577  \n",
      "17  http://www.medsur.org/symptom/10021597  \n",
      "20  http://www.medsur.org/symptom/10047290  \n"
     ]
    }
   ],
   "source": [
    "# only select triples that contains the predicate 'hasPT' and 'hasLLT'\n",
    "triples_df = triples_df[(triples_df['predicate'] == 'http://example.org/medsur.rdf#hasPT') | (triples_df['predicate'] == 'http://example.org/medsur.rdf#hasLLT')]\n",
    "print(triples_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['http://example.org/medsur.rdf#hasLLT',\n",
       "       'http://example.org/medsur.rdf#hasPT'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triples_df.predicate.unique() # looks correct"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['http://www.medsur.org/patient_10594',\n",
       "       'http://example.org/medsur.rdf#hasLLT',\n",
       "       'http://www.medsur.org/symptom/10046901'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create np array of triples [[row1], [row2], ...]\n",
    "triples = triples_df.values\n",
    "triples[0] # looks correct"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size:  (26370, 3)\n",
      "Test set size:  (2930, 3)\n"
     ]
    }
   ],
   "source": [
    "from ampligraph.evaluation import train_test_split_no_unseen\n",
    "\n",
    "test_size = int(0.1*len(triples_df))\n",
    "\n",
    "X_train, X_test = train_test_split_no_unseen(triples, test_size=test_size)\n",
    "\n",
    "print('Train set size: ', X_train.shape)\n",
    "print('Test set size: ', X_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose ComplEx as our Knowledge Graph Embedding model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.latent_features.models import ScoringBasedEmbeddingModel\n",
    "    \n",
    "# Initialize a ComplEx neural embedding model: the embedding size is k,\n",
    "# eta specifies the number of corruptions to generate per each positive,\n",
    "# scoring_type determines the scoring function of the embedding model.\n",
    "model = ScoringBasedEmbeddingModel(k=150,\n",
    "                                   eta=10,\n",
    "                                   scoring_type='ComplEx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from ampligraph.latent_features.loss_functions import get as get_loss\n",
    "from ampligraph.latent_features.regularizers import get as get_regularizer\n",
    "\n",
    "# Optimizer, loss and regularizer definition\n",
    "optim = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss = get_loss('pairwise', {'margin': 0.5})\n",
    "regularizer = get_regularizer('LP', {'p': 2, 'lambda': 1e-5})\n",
    "\n",
    "# Compilation of the model\n",
    "model.compile(loss=loss,\n",
    "              optimizer='adam',\n",
    "              entity_relation_regularizer=regularizer,\n",
    "              entity_relation_initializer='glorot_uniform')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on training and validation set\n",
    "\n",
    "history = model.fit(X_train,\n",
    "          batch_size=64, # use 1/10 of the training set as batch size\n",
    "          epochs=100,                    # Number of training epochs\n",
    "          verbose=True                  # Enable stdout messages\n",
    "          )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional step when evaluating KGEs: Define a filter so that no negative statements generated by the corruption procedure are actually positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = {'test': np.concatenate([X_train, X_test])}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = model.evaluate(X_test,\n",
    "                       use_filter=filter,\n",
    "                       corrupt_side='s,o',\n",
    "                       verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the mrr_score (mean reciprocal rank) and hits_at_n_score functions to evaluate the quality of our predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.evaluation import mr_score, mrr_score, hits_at_n_score\n",
    "\n",
    "mr = mr_score(ranks)\n",
    "mrr = mrr_score(ranks)\n",
    "\n",
    "print(\"MRR: %.2f\" % (mrr))\n",
    "print(\"MR: %.2f\" % (mr))\n",
    "\n",
    "hits_10 = hits_at_n_score(ranks, n=10)\n",
    "print(\"Hits@10: %.2f\" % (hits_10))\n",
    "hits_3 = hits_at_n_score(ranks, n=3)\n",
    "print(\"Hits@3: %.2f\" % (hits_3))\n",
    "hits_1 = hits_at_n_score(ranks, n=1)\n",
    "print(\"Hits@1: %.2f\" % (hits_1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get embeddings for PTCodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symptoms = triples_df.object.unique()\n",
    "embeddings = dict(zip(symptoms, model.get_embeddings(symptoms)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use PCA to project the embeddings from the 200 space into 2D space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_2d = PCA(n_components=2).fit_transform(np.array([i for i in embeddings.values()]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the ideal k-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "WCSS = []\n",
    "for i in range(1, 11):\n",
    "    km = KMeans(n_clusters=i, n_init=50, max_iter=500, random_state=0,init = 'k-means++')\n",
    "    km.fit(embeddings_2d )\n",
    "    WCSS.append(km.inertia_)\n",
    "\n",
    "# plot\n",
    "plt.plot(range(1, 11), WCSS, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Distortion')\n",
    "plt.show() # Based on elbow method, k should be 3."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now cluster the teams embeddings on its original 200-dimensional space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.discovery import find_clusters\n",
    "\n",
    "clustering_algorithm = KMeans(n_clusters=3, n_init=50, max_iter=500, random_state=0)\n",
    "clusters = find_clusters(symptoms, model, clustering_algorithm, mode='e')\n",
    "\n",
    "print(len(clusters))\n",
    "print(len(symptoms))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the clusters for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster-patient dictionary\n",
    "\n",
    "results = dict(zip(clusters, symptoms))\n",
    "\n",
    "with open(\"clusters_entities.txt\", 'w') as f: \n",
    "    for key, value in results.items(): \n",
    "        f.write('%s:%s\\n' % (key, value))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.DataFrame({\"symptoms\": symptoms, \n",
    "                        \"embedding1\": embeddings_2d[:, 0], \n",
    "                        \"embedding2\": embeddings_2d[:, 1],\n",
    "                        \"cluster\": \"cluster\" + pd.Series(clusters).astype(str)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2D embeddings\n",
    "def plot_clusters(hue):\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.title(\"{} embeddings\".format(hue).capitalize())\n",
    "    ax = sns.scatterplot(data=plot_df,\n",
    "                         x=\"embedding1\", y=\"embedding2\", hue=hue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(\"cluster\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
