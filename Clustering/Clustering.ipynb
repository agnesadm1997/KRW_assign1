{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import rdflib\n",
    "from rdflib import Graph, Literal, Namespace, RDF, URIRef, OWL\n",
    "from rdflib.extras.external_graph_libs import rdflib_to_networkx_digraph\n",
    "from rdflib.namespace import DC, FOAF\n",
    "\n",
    "from owlready2 import *\n",
    "from owlready2 import get_ontology\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import networkx as nx\n",
    "import networkx.algorithms.community as nx_comm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from adjustText import adjust_text\n",
    "%matplotlib inline\n",
    "\n",
    "random.seed(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data with necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('opioid_data_merged_min.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['WorldwideUniqueCaseIdentification'] = data.WorldwideUniqueCaseIdentification.astype(str)\n",
    "data['age_group'] = data.age_group.astype(str)\n",
    "data['PTCode'] = data.PTCode.astype(str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Triple extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples = []\n",
    "for _, row in data.iterrows():\n",
    "        \n",
    "    # Weight group info\n",
    "    weight_group = (row['WorldwideUniqueCaseIdentification'], \"hasWeightGroup\", row['weight_group'])\n",
    "\n",
    "    # Age group info\n",
    "    age_group = (row['WorldwideUniqueCaseIdentification'], \"hasAgeGroup\", row['age_group'])\n",
    "\n",
    "    # Gender info\n",
    "    sex = (row['WorldwideUniqueCaseIdentification'], \"hasGender\", row['sex'])\n",
    "\n",
    "    #Outcome\n",
    "    outcome = (row['WorldwideUniqueCaseIdentification'], \"hasOutcome\", row['Outcome'])\n",
    "\n",
    "    #Symptom\n",
    "    symptom = (row['WorldwideUniqueCaseIdentification'], \"hasSymptom\", row['PTCode'])\n",
    "\n",
    "    #Drug\n",
    "    drug = (row['WorldwideUniqueCaseIdentification'], \"isGivenDrug\", row['ATCode'])\n",
    "\n",
    "    #Frequency\n",
    "    frequency = (row['PTCode'], \"hasFrequency\", row['Frequency'])    \n",
    "    \n",
    "    #Side effect\n",
    "    if row['is_sideeffect'] == True:\n",
    "        side_effect = (row['WorldwideUniqueCaseIdentification'], \"hasSideEffect\", row['PTCode'])\n",
    "    else:\n",
    "        side_effect = (row['WorldwideUniqueCaseIdentification'], \"hasSideEffect\", \"None\")\n",
    "\n",
    "    triples.extend((weight_group, age_group, sex, outcome, symptom, drug, frequency, side_effect))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how they look in a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples_df = pd.DataFrame(triples, columns=[\"subject\", \"predicate\", \"object\"])\n",
    "triples_df = triples_df.drop_duplicates()\n",
    "triples_df[(triples_df.subject==\"NL-TEVA-719924ROM\")]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.evaluation import train_test_split_no_unseen\n",
    "\n",
    "X_train, X_test = train_test_split_no_unseen(np.array(triples), test_size=7930)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train set size: ', X_train.shape)\n",
    "print('Test set size: ', X_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose ComplEx as our Knowledge Graph Embedding model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.latent_features.models import ScoringBasedEmbeddingModel\n",
    "    \n",
    "# Initialize a ComplEx neural embedding model: the embedding size is k,\n",
    "# eta specifies the number of corruptions to generate per each positive,\n",
    "# scoring_type determines the scoring function of the embedding model.\n",
    "model = ScoringBasedEmbeddingModel(k=150,\n",
    "                                   eta=10,\n",
    "                                   scoring_type='ComplEx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from ampligraph.latent_features.loss_functions import get as get_loss\n",
    "from ampligraph.latent_features.regularizers import get as get_regularizer\n",
    "\n",
    "# Optimizer, loss and regularizer definition\n",
    "optim = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss = get_loss('pairwise', {'margin': 0.5})\n",
    "regularizer = get_regularizer('LP', {'p': 2, 'lambda': 1e-5})\n",
    "\n",
    "# Compilation of the model\n",
    "model.compile(loss=loss,\n",
    "              optimizer='adam',\n",
    "              entity_relation_regularizer=regularizer,\n",
    "              entity_relation_initializer='glorot_uniform')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on training and validation set\n",
    "model.fit(X_train,\n",
    "          batch_size=int(X_train.shape[0] / 10), # use 1/10 of the training set as batch size\n",
    "          epochs=200,                    # Number of training epochs\n",
    "          verbose=True                  # Enable stdout messages\n",
    "          )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional step when evaluating KGEs: Define a filter so that no negative statements generated by the corruption procedure are actually positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = {'test': np.concatenate([X_train, X_test])}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = model.evaluate(X_test,\n",
    "                       use_filter=filter,\n",
    "                       corrupt_side='s,o',\n",
    "                       verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the mrr_score (mean reciprocal rank) and hits_at_n_score functions to evaluate the quality of our predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.evaluation import mr_score, mrr_score, hits_at_n_score\n",
    "\n",
    "mr = mr_score(ranks)\n",
    "mrr = mrr_score(ranks)\n",
    "\n",
    "print(\"MRR: %.2f\" % (mrr))\n",
    "print(\"MR: %.2f\" % (mr))\n",
    "\n",
    "hits_10 = hits_at_n_score(ranks, n=10)\n",
    "print(\"Hits@10: %.2f\" % (hits_10))\n",
    "hits_3 = hits_at_n_score(ranks, n=3)\n",
    "print(\"Hits@3: %.2f\" % (hits_3))\n",
    "hits_1 = hits_at_n_score(ranks, n=1)\n",
    "print(\"Hits@1: %.2f\" % (hits_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = data.WorldwideUniqueCaseIdentification.unique()\n",
    "patient_embeddings = dict(zip(patients, model.get_embeddings(patients)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_2d = PCA(n_components=2).fit_transform(np.array([i for i in patient_embeddings.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from ampligraph.discovery import find_clusters\n",
    "\n",
    "clustering_algorithm = KMeans(n_clusters=6, n_init=50, max_iter=500, random_state=0)\n",
    "clusters = find_clusters(patients, model, clustering_algorithm, mode='e')\n",
    "\n",
    "print(len(clusters))\n",
    "print(len(patients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster-patient dictionary\n",
    "\n",
    "results = dict(zip(clusters, patients))\n",
    "\n",
    "with open(\"clusters.txt\", 'w') as f: \n",
    "    for key, value in results.items(): \n",
    "        f.write('%s:%s\\n' % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.DataFrame({\"patients\": patients, \n",
    "                        \"embedding1\": embeddings_2d[:, 0], \n",
    "                        \"embedding2\": embeddings_2d[:, 1],\n",
    "                        \"cluster\": \"cluster\" + pd.Series(clusters).astype(str)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2D embeddings\n",
    "def plot_clusters(hue):\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.title(\"{} embeddings\".format(hue).capitalize())\n",
    "    ax = sns.scatterplot(data=plot_df,\n",
    "                         x=\"embedding1\", y=\"embedding2\", hue=hue)\n",
    "    texts = []\n",
    "    adjust_text(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(\"cluster\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
