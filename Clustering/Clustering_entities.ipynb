{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import rdflib\n",
    "from rdflib import Graph, Literal, Namespace, RDF, URIRef, OWL\n",
    "from rdflib.extras.external_graph_libs import rdflib_to_networkx_digraph\n",
    "from rdflib.namespace import DC, FOAF\n",
    "\n",
    "from owlready2 import *\n",
    "from owlready2 import get_ontology\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import networkx as nx\n",
    "import networkx.algorithms.community as nx_comm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from adjustText import adjust_text\n",
    "%matplotlib inline\n",
    "\n",
    "random.seed(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv medsur.csv\n",
    "colnames = [\"subject\", \"predicate\", \"object\"]\n",
    "triples_df = pd.read_csv('medsur.csv', names=colnames, header=None) \n",
    "triples_df['object'] = triples_df['object'].str.rstrip()\n",
    "\n",
    "print(triples_df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter triples (maybe for later use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# only select triples that contains the predicate 'has_outcome'\n",
    "triples_df = triples_df[triples_df['predicate'] == 'http://example.org/medsur.rdf#hasOutcome']\n",
    "print(triples_df.head())\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create np array of triples [[row1], [row2], ...]\n",
    "triples = triples_df.values\n",
    "triples[0] # feels correct\n",
    "\n",
    "entities = np.unique(np.concatenate([triples[:, 0], triples[:, 2]]))\n",
    "entities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.evaluation import train_test_split_no_unseen\n",
    "\n",
    "test_size = int(0.2*len(triples_df))\n",
    "\n",
    "X_train, X_test = train_test_split_no_unseen(triples, test_size=test_size)\n",
    "\n",
    "print('Train set size: ', X_train.shape)\n",
    "print('Test set size: ', X_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose ComplEx as our Knowledge Graph Embedding model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.latent_features.models import ScoringBasedEmbeddingModel\n",
    "    \n",
    "# Initialize a ComplEx neural embedding model: the embedding size is k,\n",
    "# eta specifies the number of corruptions to generate per each positive,\n",
    "# scoring_type determines the scoring function of the embedding model.\n",
    "model = ScoringBasedEmbeddingModel(k=150,\n",
    "                                   eta=10,\n",
    "                                   scoring_type='ComplEx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from ampligraph.latent_features.loss_functions import get as get_loss\n",
    "from ampligraph.latent_features.regularizers import get as get_regularizer\n",
    "\n",
    "# Optimizer, loss and regularizer definition\n",
    "optim = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss = get_loss('pairwise', {'margin': 0.5})\n",
    "regularizer = get_regularizer('LP', {'p': 2, 'lambda': 1e-5})\n",
    "\n",
    "# Compilation of the model\n",
    "model.compile(loss=loss,\n",
    "              optimizer='adam',\n",
    "              entity_relation_regularizer=regularizer,\n",
    "              entity_relation_initializer='glorot_uniform')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on training and validation set\n",
    "\n",
    "history = model.fit(X_train,\n",
    "          batch_size=int(X_train.shape[0] / 10), # use 1/10 of the training set as batch size\n",
    "          epochs=200,                    # Number of training epochs\n",
    "          verbose=True                  # Enable stdout messages\n",
    "          )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional step when evaluating KGEs: Define a filter so that no negative statements generated by the corruption procedure are actually positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = {'test': np.concatenate([X_train, X_test])}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = model.evaluate(X_test,\n",
    "                       use_filter=filter,\n",
    "                       corrupt_side='s,o',\n",
    "                       verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the mrr_score (mean reciprocal rank) and hits_at_n_score functions to evaluate the quality of our predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.evaluation import mr_score, mrr_score, hits_at_n_score\n",
    "\n",
    "mr = mr_score(ranks)\n",
    "mrr = mrr_score(ranks)\n",
    "\n",
    "print(\"MRR: %.2f\" % (mrr))\n",
    "print(\"MR: %.2f\" % (mr))\n",
    "\n",
    "hits_10 = hits_at_n_score(ranks, n=10)\n",
    "print(\"Hits@10: %.2f\" % (hits_10))\n",
    "hits_3 = hits_at_n_score(ranks, n=3)\n",
    "print(\"Hits@3: %.2f\" % (hits_3))\n",
    "hits_1 = hits_at_n_score(ranks, n=1)\n",
    "print(\"Hits@1: %.2f\" % (hits_1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get embeddings for PTCodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = np.unique(np.concatenate([triples[:, 0], triples[:, 2]]))\n",
    "embeddings = dict(zip(entities, model.get_embeddings(entities)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use PCA to project the embeddings from the 200 space into 2D space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_2d = PCA(n_components=2).fit_transform(np.array([i for i in embeddings.values()]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the ideal k-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "WCSS = []\n",
    "for i in range(1, 11):\n",
    "    km = KMeans(n_clusters=i, n_init=50, max_iter=500, random_state=0,init = 'k-means++')\n",
    "    km.fit(embeddings_2d )\n",
    "    WCSS.append(km.inertia_)\n",
    "\n",
    "# plot\n",
    "plt.plot(range(1, 11), WCSS, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Distortion')\n",
    "plt.show() # Based on elbow method, k should be 3."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now cluster the teams embeddings on its original 200-dimensional space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.discovery import find_clusters\n",
    "\n",
    "clustering_algorithm = KMeans(n_clusters=3, n_init=50, max_iter=500, random_state=0)\n",
    "clusters = find_clusters(entities, model, clustering_algorithm, mode='e')\n",
    "\n",
    "print(len(clusters))\n",
    "print(len(entities))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the clusters for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster-patient dictionary\n",
    "\n",
    "results = dict(zip(clusters, entities))\n",
    "\n",
    "with open(\"clusters_entities.txt\", 'w') as f: \n",
    "    for key, value in results.items(): \n",
    "        f.write('%s:%s\\n' % (key, value))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.DataFrame({\"entities\": entities, \n",
    "                        \"embedding1\": embeddings_2d[:, 0], \n",
    "                        \"embedding2\": embeddings_2d[:, 1],\n",
    "                        \"cluster\": \"cluster\" + pd.Series(clusters).astype(str)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2D embeddings\n",
    "def plot_clusters(hue):\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.title(\"{} embeddings\".format(hue).capitalize())\n",
    "    ax = sns.scatterplot(data=plot_df,\n",
    "                         x=\"embedding1\", y=\"embedding2\", hue=hue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(\"cluster\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
