{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ampligraph.datasets import load_wn18\n",
    "from ampligraph.latent_features import ScoringBasedEmbeddingModel\n",
    "from ampligraph.evaluation import mrr_score, hits_at_n_score\n",
    "from ampligraph.latent_features.loss_functions import get as get_loss\n",
    "from ampligraph.latent_features.regularizers import get as get_regularizer\n",
    "import tensorflow as tf\n",
    "from ampligraph.evaluation import train_test_split_no_unseen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      subject  \\\n",
      "0          http://www.medsur.org/drug/N02AE01   \n",
      "1          http://www.medsur.org/drug/N02AX02   \n",
      "2  http://www.medsur.org/side_effect/10013573   \n",
      "3  http://www.medsur.org/side_effect/10003036   \n",
      "4  http://www.medsur.org/side_effect/10037211   \n",
      "\n",
      "                                         predicate  \\\n",
      "0            http://www.medsur.org/isPrescribedFor   \n",
      "1              http://www.medsur.org/hasSideEffect   \n",
      "2               http://www.medsur.org/hasFrequency   \n",
      "3               http://www.medsur.org/hasFrequency   \n",
      "4  http://www.w3.org/1999/02/22-rdf-syntax-ns#type   \n",
      "\n",
      "                                        object  \n",
      "0      http://www.medsur.org/symptom/10080284   \n",
      "1  http://www.medsur.org/side_effect/10046543   \n",
      "2                                        5.26   \n",
      "3                                           9   \n",
      "4           http://www.medsur.org/SideEffects   \n"
     ]
    }
   ],
   "source": [
    "# load csv medsur.csv\n",
    "colnames = [\"subject\", \"predicate\", \"object\"]\n",
    "triples_df = pd.read_csv('medsur.csv', names=colnames, header=None) \n",
    "\n",
    "print(triples_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['http://www.medsur.org/drug/N02AE01'\n",
      "  'http://www.medsur.org/isPrescribedFor'\n",
      "  'http://www.medsur.org/symptom/10080284 ']\n",
      " ['http://www.medsur.org/drug/N02AX02'\n",
      "  'http://www.medsur.org/hasSideEffect'\n",
      "  'http://www.medsur.org/side_effect/10046543 ']\n",
      " ['http://www.medsur.org/side_effect/10013573'\n",
      "  'http://www.medsur.org/hasFrequency' '5.26 ']\n",
      " ...\n",
      " ['http://www.medsur.org/side_effect/10024419'\n",
      "  'http://www.medsur.org/hasFrequency' 'postmarketing ']\n",
      " ['http://example.org/medsur.owl#Patients'\n",
      "  'http://www.w3.org/1999/02/22-rdf-syntax-ns#type'\n",
      "  'http://www.w3.org/2002/07/owl#Class ']\n",
      " ['http://www.medsur.org/side_effect/10020843'\n",
      "  'http://www.medsur.org/hasFrequency' 'very_rare ']]\n"
     ]
    }
   ],
   "source": [
    "# create np array of triples [[row1], [row2], ...]\n",
    "triples = triples_df.values\n",
    "print(triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size:  (4635, 3)\n",
      "Test set size:  (579, 3)\n",
      "Validation set size:  (579, 3)\n"
     ]
    }
   ],
   "source": [
    "test_size = int(0.1*len(triples_df))\n",
    "\n",
    "X_train_valid, X_test = train_test_split_no_unseen(triples, test_size=test_size)\n",
    "X_train, X_valid = train_test_split_no_unseen(X_train_valid, test_size=test_size)\n",
    "\n",
    "print('Train set size: ', X_train.shape)\n",
    "print('Test set size: ', X_valid.shape)\n",
    "print('Validation set size: ', X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "12/12 [==============================] - 6s 486ms/step - loss: 2106.7783\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 2101.6848\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 2095.3230\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 2085.5000\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 2069.5291\n",
      "Epoch 6/20\n",
      "12/12 [==============================] - 1s 78ms/step - loss: 2043.4901\n",
      "Epoch 7/20\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 2002.1842\n",
      "Epoch 8/20\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 1939.4531\n",
      "Epoch 9/20\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 1848.9843\n",
      "Epoch 10/20\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 1736.1493\n",
      "Epoch 11/20\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1617.6364\n",
      "Epoch 12/20\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1505.5505\n",
      "Epoch 13/20\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 1404.2212\n",
      "Epoch 14/20\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 1313.8359\n",
      "Epoch 15/20\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 1233.8000\n",
      "Epoch 16/20\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 1162.8298\n",
      "Epoch 17/20\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 1099.7325\n",
      "Epoch 18/20\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 1043.3824\n",
      "Epoch 19/20\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 992.7278\n",
      "Epoch 20/20\n",
      "12/12 [==============================] - 1s 78ms/step - loss: 947.0266\n",
      "20/20 [==============================] - 10s 475ms/step\n",
      "MRR: 0.372736, Hits@10: 0.518998\n"
     ]
    }
   ],
   "source": [
    "from ampligraph.latent_features.models import ScoringBasedEmbeddingModel as model_embedding\n",
    "    \n",
    "# Initialize a ComplEx neural embedding model: the embedding size is k,\n",
    "# eta specifies the number of corruptions to generate per each positive,\n",
    "# scoring_type determines the scoring function of the embedding model.\n",
    "model = ScoringBasedEmbeddingModel(k=150,\n",
    "                                   eta=10,\n",
    "                                   scoring_type='ComplEx')\n",
    "\n",
    "# Optimizer, loss and regularizer definition\n",
    "optim = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss = get_loss('pairwise', {'margin': 0.5})\n",
    "regularizer = get_regularizer('LP', {'p': 2, 'lambda': 1e-5})\n",
    "\n",
    "# Compilation of the model\n",
    "model.compile(optimizer=optim, loss=loss, entity_relation_regularizer=regularizer)\n",
    "\n",
    "# For evaluation, we can use a filter which would be used to filter out\n",
    "# positives statements created by the corruption procedure.\n",
    "# Here we define the filter set by concatenating all the positives\n",
    "filter = {'test' : np.concatenate((X_train, X_valid, X_test))}\n",
    "\n",
    "# Early Stopping callback\n",
    "checkpoint = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_{}'.format('hits10'),\n",
    "    min_delta=0,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode='max',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Fit the model on training and validation set\n",
    "model.fit(X_train,\n",
    "          batch_size=int(X_train.shape[0] / 10),\n",
    "          epochs=20,                    # Number of training epochs\n",
    "          validation_freq=20,           # Epochs between successive validation\n",
    "          validation_burn_in=100,       # Epoch to start validation\n",
    "          validation_data=X_valid,   # Validation data\n",
    "          validation_filter=filter,     # Filter positives from validation corruptions\n",
    "          callbacks=[checkpoint],       # Early stopping callback (more from tf.keras.callbacks are supported)\n",
    "          verbose=True                  # Enable stdout messages\n",
    "          )\n",
    "\n",
    "\n",
    "# Run the evaluation procedure on the test set (with filtering)\n",
    "# To disable filtering: use_filter=None\n",
    "# Usually, we corrupt subject and object sides separately and compute ranks\n",
    "ranks = model.evaluate(X_test,\n",
    "                       use_filter=filter,\n",
    "                       corrupt_side='s,o')\n",
    "\n",
    "# compute and print metrics:\n",
    "mrr = mrr_score(ranks)\n",
    "hits_10 = hits_at_n_score(ranks, n=10)\n",
    "print(\"MRR: %f, Hits@10: %f\" % (mrr, hits_10))\n",
    "# Output: MRR: 0.884418, Hits@10: 0.935500"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2967556ad1cc0b07d108fdcc876ab0a44458be2f8a9d1ccef00065e78f2b8f3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
