{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import rdflib\n",
    "from rdflib import Graph, Literal, Namespace, RDF, URIRef, OWL\n",
    "from rdflib.extras.external_graph_libs import rdflib_to_networkx_digraph\n",
    "from rdflib.namespace import DC, FOAF\n",
    "\n",
    "from owlready2 import *\n",
    "from owlready2 import get_ontology\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import networkx as nx\n",
    "import networkx.algorithms.community as nx_comm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from adjustText import adjust_text\n",
    "%matplotlib inline\n",
    "\n",
    "random.seed(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             subject  \\\n",
      "0  http://www.medsur.org/patient_NLGRUNENTHAL2018...   \n",
      "1           http://www.medsur.org/patient_NLLRB30683   \n",
      "2           http://www.medsur.org/patient_NLLRB34229   \n",
      "3          http://www.medsur.org/patient_NLLRB187444   \n",
      "4           http://www.medsur.org/patient_NLLRB18477   \n",
      "\n",
      "                                         predicate  \\\n",
      "0  http://example.org/medsur.rdf#suffersSideEffect   \n",
      "1  http://example.org/medsur.rdf#suffersSideEffect   \n",
      "2  http://example.org/medsur.rdf#suffersSideEffect   \n",
      "3  http://example.org/medsur.rdf#suffersSideEffect   \n",
      "4  http://example.org/medsur.rdf#suffersSideEffect   \n",
      "\n",
      "                                       object  \n",
      "0  http://www.medsur.org/side_effect/10053198  \n",
      "1  http://www.medsur.org/side_effect/10007697  \n",
      "2  http://www.medsur.org/side_effect/10008479  \n",
      "3  http://www.medsur.org/side_effect/10014080  \n",
      "4  http://www.medsur.org/side_effect/10002855  \n"
     ]
    }
   ],
   "source": [
    "# load csv medsur.csv\n",
    "colnames = [\"subject\", \"predicate\", \"object\"]\n",
    "triples_df = pd.read_csv('medsur.csv', names=colnames, header=None) \n",
    "triples_df['object'] = triples_df['object'].str.rstrip()\n",
    "\n",
    "print(triples_df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter triples to get side effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             subject  \\\n",
      "0  http://www.medsur.org/patient_NLGRUNENTHAL2018...   \n",
      "1           http://www.medsur.org/patient_NLLRB30683   \n",
      "2           http://www.medsur.org/patient_NLLRB34229   \n",
      "3          http://www.medsur.org/patient_NLLRB187444   \n",
      "4           http://www.medsur.org/patient_NLLRB18477   \n",
      "\n",
      "                                         predicate  \\\n",
      "0  http://example.org/medsur.rdf#suffersSideEffect   \n",
      "1  http://example.org/medsur.rdf#suffersSideEffect   \n",
      "2  http://example.org/medsur.rdf#suffersSideEffect   \n",
      "3  http://example.org/medsur.rdf#suffersSideEffect   \n",
      "4  http://example.org/medsur.rdf#suffersSideEffect   \n",
      "\n",
      "                                       object  \n",
      "0  http://www.medsur.org/side_effect/10053198  \n",
      "1  http://www.medsur.org/side_effect/10007697  \n",
      "2  http://www.medsur.org/side_effect/10008479  \n",
      "3  http://www.medsur.org/side_effect/10014080  \n",
      "4  http://www.medsur.org/side_effect/10002855  \n"
     ]
    }
   ],
   "source": [
    "# only select triples that contains the predicate 'hasSymptom'\n",
    "triples_df = triples_df[(triples_df['predicate'] == 'http://example.org/medsur.rdf#suffersSideEffect')]\n",
    "print(triples_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['http://example.org/medsur.rdf#suffersSideEffect'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triples_df.predicate.unique() # looks correct but does not feel correct"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['http://www.medsur.org/patient_ESKYOWAKIRIN2019BKK009955',\n",
       "       'http://www.medsur.org/patient_JPKYOWAKIRIN2019AKK009362',\n",
       "       'http://www.medsur.org/patient_NL002147023NVSC2019NL008459', ...,\n",
       "       'http://www.medsur.org/side_effect/10072575',\n",
       "       'http://www.medsur.org/side_effect/10072746',\n",
       "       'http://www.medsur.org/side_effect/10074170'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create np array of triples [[row1], [row2], ...]\n",
    "triples = triples_df.values\n",
    "triples[0] # feels correct\n",
    "\n",
    "entities = np.unique(np.concatenate([triples[:, 0], triples[:, 2]]))\n",
    "entities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size:  (828556, 3)\n",
      "Test set size:  (207138, 3)\n"
     ]
    }
   ],
   "source": [
    "from ampligraph.evaluation import train_test_split_no_unseen\n",
    "\n",
    "test_size = int(0.2*len(triples_df))\n",
    "\n",
    "X_train, X_test = train_test_split_no_unseen(triples, test_size=test_size)\n",
    "\n",
    "print('Train set size: ', X_train.shape)\n",
    "print('Test set size: ', X_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose ComplEx as our Knowledge Graph Embedding model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.latent_features.models import ScoringBasedEmbeddingModel\n",
    "    \n",
    "# Initialize a ComplEx neural embedding model: the embedding size is k,\n",
    "# eta specifies the number of corruptions to generate per each positive,\n",
    "# scoring_type determines the scoring function of the embedding model.\n",
    "model = ScoringBasedEmbeddingModel(k=150,\n",
    "                                   eta=10,\n",
    "                                   scoring_type='ComplEx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from ampligraph.latent_features.loss_functions import get as get_loss\n",
    "from ampligraph.latent_features.regularizers import get as get_regularizer\n",
    "\n",
    "# Optimizer, loss and regularizer definition\n",
    "optim = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss = get_loss('pairwise', {'margin': 0.5})\n",
    "regularizer = get_regularizer('LP', {'p': 2, 'lambda': 1e-5})\n",
    "\n",
    "# Compilation of the model\n",
    "model.compile(loss=loss,\n",
    "              optimizer='adam',\n",
    "              entity_relation_regularizer=regularizer,\n",
    "              entity_relation_initializer='glorot_uniform')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "12/12 [==============================] - 66s 5s/step - loss: 376629.7188\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - 63s 5s/step - loss: 376384.0312\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - 67s 6s/step - loss: 375502.8438\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - 65s 5s/step - loss: 373050.4062\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - 79s 7s/step - loss: 367611.5312\n",
      "Epoch 6/20\n",
      "12/12 [==============================] - 79s 7s/step - loss: 357322.3750\n",
      "Epoch 7/20\n",
      " 3/12 [======>.......................] - ETA: 48s - loss: 355431.0625"
     ]
    }
   ],
   "source": [
    "# Fit the model on training and validation set\n",
    "\n",
    "history = model.fit(X_train,\n",
    "          batch_size=int(X_train.shape[0] / 10), # use 1/10 of the training set as batch size\n",
    "          epochs=20,                    # Number of training epochs\n",
    "          verbose=True                  # Enable stdout messages\n",
    "          )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional step when evaluating KGEs: Define a filter so that no negative statements generated by the corruption procedure are actually positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = {'test': np.concatenate([X_train, X_test])}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = model.evaluate(X_test,\n",
    "                       use_filter=filter,\n",
    "                       corrupt_side='s,o',\n",
    "                       verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the mrr_score (mean reciprocal rank) and hits_at_n_score functions to evaluate the quality of our predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.evaluation import mr_score, mrr_score, hits_at_n_score\n",
    "\n",
    "mr = mr_score(ranks)\n",
    "mrr = mrr_score(ranks)\n",
    "\n",
    "print(\"MRR: %.2f\" % (mrr))\n",
    "print(\"MR: %.2f\" % (mr))\n",
    "\n",
    "hits_10 = hits_at_n_score(ranks, n=10)\n",
    "print(\"Hits@10: %.2f\" % (hits_10))\n",
    "hits_3 = hits_at_n_score(ranks, n=3)\n",
    "print(\"Hits@3: %.2f\" % (hits_3))\n",
    "hits_1 = hits_at_n_score(ranks, n=1)\n",
    "print(\"Hits@1: %.2f\" % (hits_1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get embeddings for PTCodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_effects = triples_df.object.unique()\n",
    "embeddings = dict(zip(entities, model.get_embeddings(side_effects)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use PCA to project the embeddings from the 200 space into 2D space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_2d = PCA(n_components=2).fit_transform(np.array([i for i in embeddings.values()]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the ideal k-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "WCSS = []\n",
    "for i in range(1, 11):\n",
    "    km = KMeans(n_clusters=i, n_init=50, max_iter=500, random_state=0,init = 'k-means++')\n",
    "    km.fit(embeddings_2d )\n",
    "    WCSS.append(km.inertia_)\n",
    "\n",
    "# plot\n",
    "plt.plot(range(1, 11), WCSS, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Distortion')\n",
    "plt.show() # Based on elbow method, k should be 3."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now cluster the teams embeddings on its original 200-dimensional space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.discovery import find_clusters\n",
    "\n",
    "clustering_algorithm = KMeans(n_clusters=3, n_init=50, max_iter=500, random_state=0)\n",
    "clusters = find_clusters(side_effects, model, clustering_algorithm, mode='e')\n",
    "\n",
    "print(len(clusters))\n",
    "print(len(side_effects))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the clusters for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster-patient dictionary\n",
    "\n",
    "results = dict(zip(clusters, entities))\n",
    "\n",
    "with open(\"clusters_entities.txt\", 'w') as f: \n",
    "    for key, value in results.items(): \n",
    "        f.write('%s:%s\\n' % (key, value))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.DataFrame({\"side_effects\": side_effects, \n",
    "                        \"embedding1\": embeddings_2d[:, 0], \n",
    "                        \"embedding2\": embeddings_2d[:, 1],\n",
    "                        \"cluster\": \"cluster\" + pd.Series(clusters).astype(str)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2D embeddings\n",
    "def plot_clusters(hue):\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.title(\"{} embeddings\".format(hue).capitalize())\n",
    "    ax = sns.scatterplot(data=plot_df,\n",
    "                         x=\"embedding1\", y=\"embedding2\", hue=hue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(\"cluster\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
